<!DOCTYPE html>
<html>

<head>
    <title>Tesla's Diary</title>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <link rel="stylesheet" href="../../../css/tufte.css" />
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>

<body>
    <header>
        <nav class="group">
            <a href="http://www.teslasdiary.com"><h2 style="text-align:left;float:left;">Tesla's Diary</h2></a>
            <a href="http://www.teslasdiary.com/about.html"><h2 style="text-align:right;float:right; margin-right:20%" class='no-print'>About</h2></a>
        </nav>
    </header>
    <br>
    
    <h1>BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies. </h1>
    <p class="subtitle">By Shihao Ji, S. V. N. Vishwanathan, Nadathur Satish, Michael J. Anderson, Pradeep Dubey</p>
    <content class="mn-demo">
        <p>
        <p>We propose BlackOut, an approximation algorithm to efficiently train massive
recurrent neural network language models (RNNLMs) with million word
vocabularies. BlackOut is motivated by using a discriminative loss, and we
describe a new sampling strategy which significantly reduces computation while
improving stability, sample efficiency, and rate of convergence. One way to
understand BlackOut is to view it as an extension of the DropOut strategy to
the output layer, wherein we use a discriminative training loss and a weighted
sampling scheme. We also establish close connections between BlackOut,
importance sampling, and noise contrastive estimation (NCE). Our experiments,
on the recently released one billion word language modeling benchmark,
demonstrate scalability and accuracy of BlackOut; we outperform the
state-of-the art, and achieve the lowest perplexity scores on this dataset.
Moreover, unlike other established methods which typically require GPUs or CPU
clusters, we show that a carefully implemented version of BlackOut requires
only 1-10 days on a single machine to train a RNNLM with a million word
vocabulary and billions of parameters on one billion of words.
</p>
<p><a href="http://arxiv.org">Donate to arXiv</a></p></p>
        
        <p><a href="http://www.arxiv.org">Donate to arxiv.org</a></p>
        <p><a target="_blank" href="http://arxiv.org/pdf/1511.06909" class='no-print'>Download</a>
        </p>
        <p><a target="_blank" href="http://arxiv.org/abs/1511.06909">Original</a>
        </p>
    </content>
    <br>
    <br>
    <br>
</body>

</html>